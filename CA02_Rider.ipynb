{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02-Rider.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing Packages"
      ],
      "metadata": {
        "id": "kqfWvS7-rAVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The first step in this process has to be importing all the necessary paclages, \n",
        "#this is now done below:\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "TSsEhMIxq-LL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cleaning the Data"
      ],
      "metadata": {
        "id": "HJgcXTWirL-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import set_matplotlib_formats\n",
        "#The next step in our analysis is cleaning the data we are given. We need to \n",
        "#make a function that can be applied to our data thus, we have the function\n",
        "# \"clean_data.\" Our function will look at our data are input all the words and \n",
        "#symbols given to it. It will then remove any words that are not in \".isaplha.\" \n",
        "#This means, any non-alphanumeric characters will not be included in our \n",
        "#dictionary.  Finally, we make a list of the 3000 most common characters in our \n",
        "#cleaned dictionary and ask it to return our final dictionary. \n",
        "\n",
        "def clean_data(start):\n",
        "\n",
        "#creating list of all words and symbols\n",
        "  initial_dictionary = []\n",
        "  email_list = [os.path.join(start,u) for u in os.listdir(start)]\n",
        "  \n",
        "  for individual in email_list:\n",
        "    with open(individual) as more:\n",
        "      for new in more:\n",
        "        words = new.split()\n",
        "        initial_dictionary += words\n",
        "  final_dict = Counter(initial_dictionary)\n",
        "  list_to_remove = list(final_dict)\n",
        "\n",
        "#removing non-alpha numeric\n",
        "\n",
        "  for piece in list_to_remove:\n",
        "    if piece.isalpha() == False:\n",
        "      del final_dict[piece]\n",
        "    elif len(piece) == 1:\n",
        "      del final_dict[piece]\n",
        "\n",
        "#3000 most common\n",
        "  final_dict = final_dict.most_common(3000)\n",
        "  return final_dict"
      ],
      "metadata": {
        "id": "V-Fk5qeHrVfN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Importing Files"
      ],
      "metadata": {
        "id": "4Dx8dhlx-Yk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#below I am going to import my data and then test it with a print statement \n",
        "#in my fucntion to see if the function is working properly"
      ],
      "metadata": {
        "id": "P2rHRMe8wnRf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeTVlaYdt8fT",
        "outputId": "242340ed-b176-4c23-ddfd-8bf878ceb2c9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mails = '/content/drive/My Drive/Colab Notebooks/test-mails'\n",
        "train_mails = '/content/drive/My Drive/Colab Notebooks/train-mails'"
      ],
      "metadata": {
        "id": "j3eJ2kSkuXHv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Showing how it works"
      ],
      "metadata": {
        "id": "cuCuvsJr-b9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After importing the train mails and test mails, I ran a print statement in my \n",
        "#clean_data function to see what came out. Here is a small sub-section of that dictionary:\n",
        "\n",
        "#[('order', 1414), ('address', 1299), ('report', 1217), ('mail', 1133), ('language', 1099), ('send', 1080), ('email', 1066), ('program', 1009), ('our', 991), ('list', 946), ('one', 921), ('name', 883), ('receive', 826), ('free', 801), ('money', 797), ('work', 756), ('information', 684), ('business', 669), ('please', 657)........\n",
        "\n",
        "#After tesitng this print statement, I then put my return statement back to keep \n",
        "#it correct"
      ],
      "metadata": {
        "id": "GH_ACt1Duo2U"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Extracting a Features Matrix"
      ],
      "metadata": {
        "id": "Z9Qza-H9-ewj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following function will extract the training and testing data set.\n",
        "#The output will be our feature matrix and the train labels. It also determines\n",
        "#if the email is spam or not. \n",
        "\n",
        "def extract(email_dir):\n",
        "  #this is a list of all the separte files that were in the folder given to us\n",
        "  new_files = [os.path.join(email_dir,fi) for fi in os.listdir(email_dir)]\n",
        "\n",
        "#here we are creating a feature matrix with 3000 columns and the number of rows\n",
        "#is equal to the number of email files\n",
        "  feature_matrix = np.zeros((len(new_files),3000))\n",
        "\n",
        "#now we create a set of empy labels based on the number of email files\n",
        "  train_lab = np.zeros(len(new_files))\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "\n",
        "#here we parse through our listed files\n",
        "  for fil in new_files:\n",
        "    with open(fil) as u:\n",
        "    #Enumerate will keep track of how many iterations have occured in our loop\n",
        "      for i, line in enumerate(u):\n",
        "        if i ==2:\n",
        "          #Now we want to split each of our lines into words \n",
        "          #We will call this list \"words_list\"\n",
        "          words = line.split()\n",
        "          #creaitng wordID\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "\n",
        "            #here we are populating our feature matrix with the count of each word\n",
        "                feature_matrix[docID,wordID] = words.count(word)\n",
        "\n",
        "#splitting based on the / in our files\n",
        "#then we are determining the last token \n",
        "      train_lab[docID] = 0;\n",
        "      Tokens = fil.split('/')\n",
        "      fin_Token = Tokens[len(Tokens)-1]\n",
        "\n",
        "#if the last token is a spam message, the email gets moved and the word count is changed\n",
        "\n",
        "      if fin_Token.startswith(\"spmsg\"):\n",
        "        train_lab[docID] = 1;\n",
        "        count = count + 1\n",
        "        #we then create a new ID for the email\n",
        "      docID = docID + 1\n",
        "  return feature_matrix, train_lab"
      ],
      "metadata": {
        "id": "mpNwTqCCuvRh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Final Steps"
      ],
      "metadata": {
        "id": "9tS3UP2R-i0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#below we will do our final process of processing the train and test data and then determining \n",
        "#how accurate our model is for determining if our email is spam or not\n",
        "\n",
        "dictionary = clean_data(train_mails)\n",
        "\n",
        "#this will use our extract function on our train mails data set and our test mails dataset\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract(train_mails)\n",
        "test_fm, test_lab = extract(test_mails)\n",
        "\n",
        "#using the Gaussian model \n",
        "model = GaussianNB()\n",
        "\n",
        "#First we train our model\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "\n",
        "#then we use our trained model to predict if it is spam or not\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "prediction = model.predict(test_fm)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "#lastly, we want to print our accuracy score to determine how well our model works\n",
        "print (accuracy_score(test_lab, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBusBNvtyaFS",
        "outputId": "44c2d99e-a42f-4371-c57d-2dd0d0748129"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QH99nE4H7FTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}